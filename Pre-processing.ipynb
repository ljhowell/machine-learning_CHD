{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook includes all relevant functions required for pre-processing of the data before the machine learning algorithms are implemented \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import relevant modules \n",
    "#import relevant libraries \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the dataset \n",
    "dataset = pd.read_csv('framingham.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Selecting features - dropping uninteresting columns'''\n",
    "\n",
    "def chose_features(dataset, features=dataset.columns, n_features = -1, v=1, vv =0):\n",
    "    '''Return reduced dataset with only chosen columns\n",
    "    - dataset: pandas dataframe of dataset to have columns chosen\n",
    "    - features (optional, default = all features): list of strings matching features to keep\n",
    "    - n_features (optional) - if specified, the top n features from the scaled list is chosen: \n",
    "    ['glucose', 'age', 'totChol', 'cigsPerDay', 'diaBP', 'prevalentHyp',\n",
    "        'diabetes', 'BPMeds', 'male', 'BMI', 'prevalentStroke',\n",
    "        'education', 'heartRate', 'currentSmoker'],\n",
    "    - v (optional) - Verbose (default 1) int 0 or 1. Print no. of features kept and lost \n",
    "    - vv (optional) - Very verbose (default 0) int 0 or 1. Print list of chosen and rejected features\n",
    "    '''\n",
    "                \n",
    "    print('Now selecting chosen features....')\n",
    "    \n",
    "    if n_features != -1:\n",
    "        if n_features > len(dataset.columns):\n",
    "            print('WARNING: chose_features has an error: n_features must be less than no. columns')\n",
    "            return(-1)\n",
    "        else:\n",
    "            ordered_f = ['TenYearCHD','glucose', 'age', 'totChol', 'cigsPerDay', 'diaBP', 'prevalentHyp',\n",
    "            'diabetes', 'BPMeds', 'male', 'BMI', 'sysBP','prevalentStroke',\n",
    "            'education', 'heartRate', 'currentSmoker']\n",
    "            features = ordered_f[0:n_features]\n",
    "\n",
    "    if v == 1: \n",
    "        print('\\t * Number of features: ', len(features))\n",
    "        print('\\t * Number of dropped features: ', len(dataset.columns) - len(features))\n",
    "        \n",
    "    if vv == 1:\n",
    "        print('\\t * Chosen features: ', features)\n",
    "        print('\\t * Dropped features: ',[col for col in dataset.columns if col not in features])\n",
    "    print('')\n",
    "    \n",
    "    return dataset.copy()[features] #reduced dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Dealing with missing values'''\n",
    "\n",
    "#Method 1: Drop missing values\n",
    "def drop_missing(dataset):\n",
    "    '''Drop rows with any missing values and return dataset with dropped rows. Prints number and percentage of rows dropped\n",
    "    - Dataset: pandas Dataframe\n",
    "    '''\n",
    "    print('Now dropping rows with missing values....')\n",
    "    dataset2 = dataset.copy().dropna().reset_index(drop=True)\n",
    "    lost = len(dataset) - len(dataset2)\n",
    "    print('\\t * Dropped {} rows {:.1f}%. {} rows remaining\\n'.format(lost,lost/len(dataset)*100,len(dataset2)))\n",
    "    return dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_normalize(dataset):\n",
    "    '''\n",
    "    Normalise all features in a dataframe between -1 and 1 and return normalised dataframe.\n",
    "    This is one method of feature scaling that may aid the performace of some ML algorithms\n",
    "    Normalisation: (feature - mean)/range\n",
    "    '''\n",
    "\n",
    "    for feature in dataset:\n",
    "        \n",
    "        fmean = np.mean(dataset[feature])\n",
    "        frange = np.amax(dataset[feature]) - np.amin(dataset[feature])\n",
    "\n",
    "        #Vector Subtraction\n",
    "        dataset[feature] = dataset[feature] - fmean\n",
    "        #Vector Division\n",
    "        dataset[feature] = dataset[feature] / frange\n",
    "\n",
    "    return dataset\n",
    "\n",
    "##e.g.\n",
    "#dataset_n = mean_normalize(dataset.copy())\n",
    "#dataset_n.head()\n",
    "\n",
    "##I then found there were some build in normalisation/ scaling modules in sklearn.preprocessing so tried some of these\n",
    "\n",
    "\n",
    "def scale_data(data, method='std'):\n",
    "    '''Return dataset scaled by MinMaxScalar or StandardScalar methods from sklearn.preprocessing\n",
    "    - data: pandas dataframe of data to be scaled\n",
    "    - method (optional): str of either 'minmax' for MinMaxScalar or 'std' for StandardScaler (default arg)\n",
    "    '''\n",
    "    from sklearn import preprocessing\n",
    "    \n",
    "    if method == 'minmax':\n",
    "        scaler_minmax = preprocessing.MinMaxScaler((0,1))\n",
    "        return pd.DataFrame(scaler_minmax.fit_transform(data.copy()),columns=data.columns) \n",
    "    \n",
    "    elif method == 'std':\n",
    "        scaler_std = preprocessing.StandardScaler() #with_std=False\n",
    "        return pd.DataFrame(scaler_std.fit_transform(dataset.copy()),columns=dataset.columns)\n",
    "    \n",
    "    else:\n",
    "        print('\\nscale_data encountered a failure!!\\n')\n",
    "        return(-1)\n",
    "\n",
    "##e.g.\n",
    "##scale_data(dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(dataset,dep_var='TenYearCHD', test_size = 0.2, v = 1):\n",
    "    '''Split the dataset, return X_train, X_test, y_train, y_test as Pandas Dataframes\n",
    "    - dataset: Pandas Dataframe. Data to split into training and test data\n",
    "    - dep_var (optional, default = 'TenYearCHD'): string. Name of column to be dependant variable\n",
    "    - test_size (optional, default = 0.2): float (0.0-1.0). Proportion of total data to make up test set.\n",
    "    '''\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    y = dataset[dep_var]\n",
    "    X = dataset.drop([dep_var], axis = 1)\n",
    "    if v == 1: \n",
    "        print('Splitting data set into {}% training, {}% test dataset....'.format(100*(1-test_size),100*test_size))\n",
    "        \n",
    "    return train_test_split(X, y, test_size = test_size, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross - validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is used to asses the predictive performance of the models and to judge how they will perform outside the sample to a new dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
