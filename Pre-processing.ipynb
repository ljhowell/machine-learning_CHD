{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook includes all relevant functions required for pre-processing of the data before the machine learning algorithms are implemented \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import relevant modules \n",
    "#import relevant libraries \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the dataset \n",
    "dataset = pd.read_csv('framingham.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Selecting features - dropping uninteresting columns'''\n",
    "\n",
    "def chose_features(dataset, features=dataset.columns, n_features = -1, v=1, vv =0):\n",
    "    '''Return reduced dataset with only chosen columns\n",
    "    - dataset: pandas dataframe of dataset to have columns chosen\n",
    "    - features (optional, default = all features): list of strings matching features to keep\n",
    "    - n_features (optional) - if specified, the top n features from the scaled list is chosen: \n",
    "    ['glucose', 'age', 'totChol', 'cigsPerDay', 'diaBP', 'prevalentHyp',\n",
    "        'diabetes', 'BPMeds', 'male', 'BMI', 'prevalentStroke',\n",
    "        'education', 'heartRate', 'currentSmoker'],\n",
    "    - v (optional) - Verbose (default 1) int 0 or 1. Print no. of features kept and lost \n",
    "    - vv (optional) - Very verbose (default 0) int 0 or 1. Print list of chosen and rejected features\n",
    "    '''\n",
    "                \n",
    "    print('Now selecting chosen features....')\n",
    "    \n",
    "    if n_features != -1:\n",
    "        if n_features > len(dataset.columns):\n",
    "            print('WARNING: chose_features has an error: n_features must be less than no. columns')\n",
    "            return(-1)\n",
    "        else:\n",
    "            ordered_f = ['TenYearCHD','glucose', 'age', 'totChol', 'cigsPerDay', 'diaBP', 'prevalentHyp',\n",
    "            'diabetes', 'BPMeds', 'male', 'BMI', 'sysBP','prevalentStroke',\n",
    "            'education', 'heartRate', 'currentSmoker']\n",
    "            features = ordered_f[0:n_features]\n",
    "\n",
    "    if v == 1: \n",
    "        print('\\t * Number of features: ', len(features))\n",
    "        print('\\t * Number of dropped features: ', len(dataset.columns) - len(features))\n",
    "        \n",
    "    if vv == 1:\n",
    "        print('\\t * Chosen features: ', features)\n",
    "        print('\\t * Dropped features: ',[col for col in dataset.columns if col not in features])\n",
    "    print('')\n",
    "    \n",
    "    return dataset.copy()[features] #reduced dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Dealing with missing values'''\n",
    "\n",
    "#Method 1: Drop missing values\n",
    "def drop_missing(dataset):\n",
    "    '''Drop rows with any missing values and return dataset with dropped rows. Prints number and percentage of rows dropped\n",
    "    - Dataset: pandas Dataframe\n",
    "    '''\n",
    "    print('Now dropping rows with missing values....')\n",
    "    dataset2 = dataset.copy().dropna().reset_index(drop=True)\n",
    "    lost = len(dataset) - len(dataset2)\n",
    "    print('\\t * Dropped {} rows {:.1f}%. {} rows remaining\\n'.format(lost,lost/len(dataset)*100,len(dataset2)))\n",
    "    return dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_normalize(dataset):\n",
    "    '''\n",
    "    Normalise all features in a dataframe between -1 and 1 and return normalised dataframe.\n",
    "    This is one method of feature scaling that may aid the performace of some ML algorithms\n",
    "    Normalisation: (feature - mean)/range\n",
    "    '''\n",
    "\n",
    "    for feature in dataset:\n",
    "        \n",
    "        fmean = np.mean(dataset[feature])\n",
    "        frange = np.amax(dataset[feature]) - np.amin(dataset[feature])\n",
    "\n",
    "        #Vector Subtraction\n",
    "        dataset[feature] = dataset[feature] - fmean\n",
    "        #Vector Division\n",
    "        dataset[feature] = dataset[feature] / frange\n",
    "\n",
    "    return dataset\n",
    "\n",
    "##e.g.\n",
    "#dataset_n = mean_normalize(dataset.copy())\n",
    "#dataset_n.head()\n",
    "\n",
    "##I then found there were some build in normalisation/ scaling modules in sklearn.preprocessing so tried some of these\n",
    "\n",
    "\n",
    "def scale_data(data, method='std'):\n",
    "    '''Return dataset scaled by MinMaxScalar or StandardScalar methods from sklearn.preprocessing\n",
    "    - data: pandas dataframe of data to be scaled\n",
    "    - method (optional): str of either 'minmax' for MinMaxScalar or 'std' for StandardScaler (default arg)\n",
    "    '''\n",
    "    from sklearn import preprocessing\n",
    "    \n",
    "    if method == 'minmax':\n",
    "        scaler_minmax = preprocessing.MinMaxScaler((0,1))\n",
    "        return pd.DataFrame(scaler_minmax.fit_transform(data.copy()),columns=data.columns) \n",
    "    \n",
    "    elif method == 'std':\n",
    "        scaler_std = preprocessing.StandardScaler() #with_std=False\n",
    "        return pd.DataFrame(scaler_std.fit_transform(dataset.copy()),columns=dataset.columns)\n",
    "    \n",
    "    else:\n",
    "        print('\\nscale_data encountered a failure!!\\n')\n",
    "        return(-1)\n",
    "\n",
    "##e.g.\n",
    "##scale_data(dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(dataset,dep_var='TenYearCHD', test_size = 0.2, v = 1):\n",
    "    '''Split the dataset, return X_train, X_test, y_train, y_test as Pandas Dataframes\n",
    "    - dataset: Pandas Dataframe. Data to split into training and test data\n",
    "    - dep_var (optional, default = 'TenYearCHD'): string. Name of column to be dependant variable\n",
    "    - test_size (optional, default = 0.2): float (0.0-1.0). Proportion of total data to make up test set.\n",
    "    '''\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    y = dataset[dep_var]\n",
    "    X = dataset.drop([dep_var], axis = 1)\n",
    "    if v == 1: \n",
    "        print('Splitting data set into {}% training, {}% test dataset....'.format(100*(1-test_size),100*test_size))\n",
    "        \n",
    "    return train_test_split(X, y, test_size = test_size, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross - validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is used to asses the predictive performance of the models and to judge how they will perform outside the sample to a new dataset \n",
    "\n",
    "The main method is the k-fold validation method, which follows the general procedure:\n",
    "1. shuffle dataset randomly\n",
    "2. split dataset into k groups\n",
    "3. For each unique group \n",
    "    - take the group as a hold out or test data set \n",
    "    - take the remaining groups as a training set \n",
    "    - fit a model on the training set and evaluate it on the test set \n",
    "    - retain the evaluation score and discard the model \n",
    "4. Summarize the skill of the model using the sample model evaluation score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation has a similar purpose to accuracy, and require the model fit to be specified, therefore I could not create it as its own function since you cannot specify a funciton where the estimatior parameter is placed.\n",
    "\n",
    "Consequently I created this line of code to add into your function for each model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation score for 3 splits:\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3c149dc62bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCrossvalidation score for 3 splits:\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#where model_name is replaced by whatever you have defined the model fit as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "print('\\nCrossvalidation score for 3 splits:\\n')   \n",
    "print(cross_val_score(model_name, X_train, y_train, cv = 3))\n",
    "\n",
    "#where model_name is replaced by whatever you have defined the model fit as \n",
    "#For example in the K-neighbors section I have defined the model_name as KN. (see k_neighbors function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any further explanation is required about these functions they have been implemented within the k_neighbors notebook or ask Lewis or Ellie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
