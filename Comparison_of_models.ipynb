{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to compare all six models that have been investigated:\n",
    "- K-neighbors \n",
    "- Decision Trees\n",
    "- Logistic Regression \n",
    "- Linear SVM\n",
    "- Random Forests \n",
    "- Multilayer perceptrons \n",
    "\n",
    "Definitions: using a confusion matrix the following measures of success can be calculated: \n",
    "\n",
    "TP = true positives\n",
    "TN = true negatives \n",
    "FP = false positives \n",
    "FN = false negatives \n",
    "\n",
    "Accuracy: Overall how often is the classifier correct? (TP+TN)/total\n",
    "\n",
    "Precision: When predicted yes, how often is it correct?\n",
    "(TP/predicted yes)\n",
    "\n",
    "Recall:  When it's actually yes, how often does it predict yes?\n",
    "TP/actual yes \n",
    "\n",
    "f1_score: A weighted average of the true positive rate (recall) and precision \n",
    "\n",
    "ROC Curve: This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing_ml as pp\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning,UndefinedMetricWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test dataset and framingham score system \n",
    "dataset = pd.read_csv('framingham.csv')\n",
    "df_fram = pd.read_csv(\"Framingham Score/fram_risk.csv\",index_col=\"index\")[['TenYearCHD','fram_percent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance algorithm from Logistic Regression kernal \n",
    "def cross_val(model, X, Y, scoring = ['accuracy','precision','recall','f1'], cv=5):\n",
    "    \n",
    "   #See https://scikit-learn.org/stable/modules/model_evaluation.html for list of allowed scoring params \n",
    "    from sklearn.model_selection import cross_val_score \n",
    "    \n",
    "    print('\\nCrossvalidation score for {} splits:\\n'.format(cv))   \n",
    "    \n",
    "    cv_results = pd.DataFrame(columns=scoring)\n",
    "    for measure in scoring:\n",
    "        cv_results[measure] = cross_val_score(model, X, Y, scoring = measure, cv=cv)\n",
    "    \n",
    "    #print(\"Cross validation Accuracy:\\n\", cv_results.mean(),\"\\nCross validation STDEV:\\n\" , cv_results.std())\n",
    "    return cv_results.mean()\n",
    "\n",
    "def performance(model,X_test,y_test,cutoff=0.5,v=0,output='Accuracy'):\n",
    "    from sklearn.preprocessing import binarize\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "    if model == 'fram': # cheap and dirty fix for framingham data needing different processing\n",
    "        y_pred = X_test > cutoff*100 \n",
    "    else:\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        y_pred = binarize(y_pred,cutoff)[:,1]\n",
    "    \n",
    "    if v == 1: \n",
    "        print('\\n========\\nCuttoff: ',cutoff)\n",
    "        print('Confusion Matrix:')\n",
    "        cm=confusion_matrix(y_test,y_pred)\n",
    "        conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "        print(confusion_matrix(y_test,y_pred))\n",
    "    \n",
    "        print('Accuracy: {:.2f}%'.format(accuracy_score(y_test,y_pred)*100))\n",
    "        print('f1 score: {:.2f}%'.format(f1_score(y_test,y_pred)*100))\n",
    "        print('precision score: {:.2f}%'.format(precision_score(y_test,y_pred)*100))\n",
    "        print('recall score: {:.2f}%'.format(recall_score(y_test,y_pred)*100))\n",
    "        \n",
    "        print(classification_report(y_test,y_pred))\n",
    "    \n",
    "\n",
    "    if output == 'Accuracy':\n",
    "         return accuracy_score(y_test,y_pred)\n",
    "    elif output == 'Precision':\n",
    "         return precision_score(y_test,y_pred)\n",
    "    elif output == 'Recall':\n",
    "         return recall_score(y_test,y_pred)\n",
    "    elif output == 'f1_score':\n",
    "         return f1_score(y_test,y_pred)\n",
    "    elif output == 'All':\n",
    "        a = [accuracy_score(y_test,y_pred), precision_score(y_test,y_pred),\n",
    "                      recall_score(y_test,y_pred), f1_score(y_test,y_pred)]\n",
    "        return a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for the Framingham model currently used by doctors \n",
    "def fram_analysis(df_fram, v=0):\n",
    "    range_x = np.arange(0.05, 1, 0.02)\n",
    "    results_fram =[]\n",
    "    \n",
    "    for cutoff in range_x:\n",
    "        results_fram.append(performance('fram', df_fram['fram_percent'], df_fram['TenYearCHD'], cutoff=cutoff, output='All'))\n",
    "    results_fram = pd.DataFrame(results_fram, columns=['accuracy', 'precision', 'recall', 'f1'])\n",
    "    \n",
    "    return results_fram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function created with the optimal conditions for the k_neighbors algorithm - SMOTE still needed to add\n",
    "def k_neighbors_opt(dataset, v=0):\n",
    "    '''Do k_neighbors  fitting and print information about the success of the fitting\n",
    "    dataset = dataset which will be used to train and test data\n",
    "    v (optional, default = 0): int (0 or 1) verbose'''\n",
    "    \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    print('\\nOptimising parameters...\\n')\n",
    "    \n",
    "    dataset_t = pp.chose_features(dataset, n_features=12) # choose n features\n",
    "    dataset_t = pp.drop_missing(dataset_t) # drop missing rows\n",
    "    dataset_t = pp.scale_data(dataset_t, 'standard', v=1 ) #scale data using minmax function \n",
    "    X_train, X_test, y_train, y_test = pp.smote(dataset_t) # split dataset\n",
    "    \n",
    "    print('\\nCalculating K_neighbors ...\\n')\n",
    "    KN = KNeighborsClassifier(n_neighbors =16)\n",
    "    KN.fit(X_train, y_train)\n",
    "    y_pred = KN.predict(X_test)\n",
    "    \n",
    "    if v == 1:\n",
    "        print('\\nRunning K_neighbors algorithm...\\n')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('\\nAccuracy is:', accuracy)\n",
    "        classification = classification_report(y_test, y_pred)\n",
    "        print(classification)\n",
    "        \n",
    "        print('Calculating metrics for ranging cutoff...')\n",
    "        range_x = np.arange(0.05, 1, 0.02)\n",
    "        results_KN = []\n",
    "\n",
    "        for cutoff in range_x:\n",
    "            results_KN.append(performance(KN, X_test, y_test,cutoff=cutoff, output='All'))\n",
    "        results_KN = pd.DataFrame(results_KN,columns=['accuracy','precision','recall','f1'])\n",
    "    \n",
    "    #probability score for ROC curve:\n",
    "    y_pred_prob_yes=KN.predict_proba(X_test)\n",
    "    \n",
    "    #confusion matrix:\n",
    "    cm=confusion_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "    return results_KN, y_pred_prob_yes, y_test, cm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function created with optimised SVM algorithm - SMOTE still needs to be added \n",
    "def SVM_opt(dataset, v=0):\n",
    "    '''Do linear support vector machine fitting and print information about the success of the fitting \n",
    "    dataset = dataset which will be used to train and test data \n",
    "    v (optional, default =0): int (0 or 1) verbose'''\n",
    "    \n",
    "    print('\\nOptimising parameters...\\n')\n",
    "    dataset_t = pp.chose_features(dataset, n_features=12)\n",
    "    dataset_t = pp.drop_missing(dataset_t) # drop missing rows\n",
    "    dataset_t = pp.scale_data(dataset_t, 'minmax', v=1)\n",
    "    X_train, X_test, y_train, y_test = pp.smote(dataset_t) # split dataset\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    print('\\nCalculating Support vector machine ..\\n')\n",
    "    SVM = SVC(C=3, random_state=0, probability=True)\n",
    "    SVM.fit(X_train, y_train) \n",
    "    y_pred = SVM.predict(X_test)\n",
    "    \n",
    "    if v == 1: \n",
    "\n",
    "        print('Running support vector machine...')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('\\nAccuracy is:', accuracy)\n",
    "        classification = classification_report(y_test, y_pred)\n",
    "        print(classification)\n",
    "        \n",
    "        print('\\nCalculating metrics for ranging classification...')\n",
    "        range_x = np.arange(0.05, 1, 0.02)\n",
    "        results_SVM =[]\n",
    "        for cutoff in range_x:\n",
    "            results_SVM.append(performance(SVM, X_test, y_test,cutoff=cutoff, output='All'))\n",
    "        results_SVM = pd.DataFrame(results_SVM,columns=['accuracy','precision','recall','f1']) \n",
    "\n",
    "    #probability score for ROC curve:    \n",
    "    y_pred_prob_yes=SVM.predict_proba(X_test)\n",
    "    \n",
    "    #Confusion matrix:\n",
    "    cm=confusion_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "    return results_SVM, y_pred_prob_yes, y_test, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function created with optimised MLP algorithm - SMOTE still needed to add\n",
    "def multilayer_opt(dataset, v=0):\n",
    "    '''Do multilayer perceptron fitting and print information about the success of the fitting\n",
    "    dataset = dataset which will be used to train and test data\n",
    "    v (optional, default = 0): int (0 or 1) verbose'''\n",
    "    print('\\nOptimising parameters...\\n')\n",
    "    dataset_t = pp.chose_features(dataset, n_features=12) # choose n features\n",
    "    dataset_t = pp.drop_missing(dataset_t) # drop missing rows\n",
    "    dataset_t = pp.scale_data(dataset_t, 'standard', v=1)\n",
    "    X_train, X_test, y_train, y_test = pp.smote(dataset_t) # split dataset\n",
    "    \n",
    "    print('\\nCalculating multilayer perceptron ...\\n')\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    MLP = MLPClassifier(hidden_layer_sizes=(1,), activation= 'identity', solver='sgd', alpha=0.05, random_state=0)\n",
    "    MLP.fit(X_train, y_train) \n",
    "    y_pred = MLP.predict(X_test)\n",
    "    \n",
    "    if v == 1:\n",
    "        print('Running multilayer perceptrons...')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('\\nAccuracy is:', accuracy)\n",
    "        classification = classification_report(y_test, y_pred)\n",
    "        print(classification)\n",
    "        \n",
    "        print('Calculating metrics for ranging cutoff...')\n",
    "        results_MLP = []\n",
    "        range_x = np.arange(0.05, 1, 0.02)\n",
    "        for cutoff in range_x:\n",
    "            results_MLP.append(performance(MLP, X_test, y_test,cutoff=cutoff, output='All'))\n",
    "        results_MLP = pd.DataFrame(results_MLP,columns=['accuracy','precision','recall','f1'])\n",
    "        \n",
    "\n",
    "    #probability score for ROC curve:    \n",
    "    y_pred_prob_yes=MLP.predict_proba(X_test)\n",
    "    \n",
    "    #confusion matrix:\n",
    "    cm=confusion_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "    return results_MLP, y_pred_prob_yes, y_test, cm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function created with optimised RF algorithm \n",
    "def RF_opt(dataset, v = 0, r_state = 0):\n",
    "    '''Random forest classifier used to machine learn, fit the data and print information about the success of the algorithm.\n",
    "    dataset = dataset which will be used to train and test data\n",
    "    v (optional, default = 0): int (0 or 1) verbose'''\n",
    "    print('\\nOptimising parameters...\\n')\n",
    "    features = ['sysBP', 'age', 'cigsPerDay', 'totChol', 'diaBP', 'prevalentHyp', 'diabetes', 'BPMeds', 'male', 'BMI', 'TenYearCHD']\n",
    "    #dataset_t = pp.outliers(dataset)\n",
    "    dataset_t = pp.chose_features(dataset, features = features, n_features = -1)\n",
    "    dataset_t = pp.drop_missing(dataset_t)\n",
    "    X_train, X_test, y_train, y_test = pp.smote(dataset_t) # split dataset\n",
    "    \n",
    "    print('\\nCalculating Random Forests ...\\n')\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier #Importing random forest module\n",
    "    from sklearn.preprocessing import binarize\n",
    "    \n",
    "    RF = RandomForestClassifier(n_estimators = 120, bootstrap = True, random_state = r_state)\n",
    "    RF.fit(X_train, y_train)\n",
    "    \n",
    "    cut_off = 0.35\n",
    "    \n",
    "    RF_preds = RF.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (RF_preds >= cut_off).astype('int')\n",
    "    \n",
    "    if v == 1:\n",
    "        print('Running Random Forests ...')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('\\nAccuracy is:', accuracy)\n",
    "        classification = classification_report(y_test, y_pred)\n",
    "        print(classification)\n",
    "        \n",
    "        print('Calculating metrics for ranging cutoff...')\n",
    "        results_RF = []\n",
    "        range_x = np.arange(0.05, 1, 0.02)\n",
    "        for cutoff in range_x:\n",
    "            results_RF.append(performance(RF, X_test, y_test,cutoff=cutoff, output='All'))\n",
    "        results_RF = pd.DataFrame(results_RF,columns=['accuracy','precision','recall','f1'])\n",
    "        \n",
    "\n",
    "    #probability score for ROC curve:    \n",
    "    y_pred_prob_yes=RF.predict_proba(X_test)\n",
    "    \n",
    "    #confusion matrix:\n",
    "    cm=confusion_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "    return results_RF, y_pred_prob_yes, y_test, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function created with optimised RF algorithm \n",
    "def DT_opt(dataset, v = 0, r_state = 0):\n",
    "    '''Decision Tree classifier used to machine learn, fit the data and print information about the success of the algorithm.\n",
    "    dataset = dataset which will be used to train and test data\n",
    "    v (optional, default = 0): int (0 or 1) verbose'''\n",
    "    print('\\nOptimising parameters...\\n')\n",
    "    features = ['sysBP', 'age', 'cigsPerDay', 'totChol', 'diaBP', 'prevalentHyp', 'diabetes', 'BPMeds', 'male', 'BMI', 'TenYearCHD']\n",
    "    #dataset_t = pp.outliers(dataset)\n",
    "    dataset_t = pp.chose_features(dataset, features = features, n_features = -1)\n",
    "    dataset_t = pp.drop_missing(dataset_t)\n",
    "    X_train, X_test, y_train, y_test = pp.smote(dataset_t) # split dataset\n",
    "    \n",
    "    print('\\nCalculating Decision Trees ...\\n')\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeClassifier #Importing decision tree classifier\n",
    "    from sklearn.preprocessing import binarize\n",
    "    \n",
    "    DT = DecisionTreeClassifier(random_state = r_state)\n",
    "    DT.fit(X_train, y_train)\n",
    "    \n",
    "    cut_off = 0.35\n",
    "    \n",
    "    DT_preds = DT.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (DT_preds >= cut_off).astype('int')\n",
    "    \n",
    "    if v == 1:\n",
    "        print('Running Decision Trees ...')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('\\nAccuracy is:', accuracy)\n",
    "        classification = classification_report(y_test, y_pred)\n",
    "        print(classification)\n",
    "        \n",
    "        print('Calculating metrics for ranging cutoff...')\n",
    "        results_DT = []\n",
    "        range_x = np.arange(0.05, 1, 0.02)\n",
    "        for cutoff in range_x:\n",
    "            results_DT.append(performance(DT, X_test, y_test, cutoff=cutoff, output='All'))\n",
    "        results_DT = pd.DataFrame(results_DT,columns=['accuracy','precision','recall','f1'])\n",
    "        \n",
    "\n",
    "    #probability score for ROC curve:    \n",
    "    y_pred_prob_yes=DT.predict_proba(X_test)\n",
    "    \n",
    "    #confusion matrix:\n",
    "    cm=confusion_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "    return results_DT, y_pred_prob_yes, y_test, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function created with optimised RF algorithm \n",
    "def LR_opt(dataset, v = 0, r_state = 0):\n",
    "    '''Logistic Regression used to machine learn, fit the data and print information about the success of the algorithm.\n",
    "    dataset = dataset which will be used to train and test data\n",
    "    v (optional, default = 0): int (0 or 1) verbose'''\n",
    "    print('\\nOptimising parameters...\\n')\n",
    "    \n",
    "    from statsmodels.tools import add_constant as add_constant\n",
    "    dataset_t = add_constant(dataset)\n",
    "\n",
    "    features = ['TenYearCHD','const','sysBP', 'glucose', 'age', 'totChol', 'cigsPerDay', 'diaBP', 'prevalentHyp',\n",
    "                'diabetes', 'BPMeds', 'male', 'BMI',\n",
    "                'education', 'heartRate'] \n",
    "    #dataset_t = pp.outliers(dataset)\n",
    "    dataset_t = pp.chose_features(dataset_t, features = features)\n",
    "    dataset_t = pp.drop_missing(dataset_t)\n",
    "    dataset_t = pp.scale_data(dataset_t, 'standard')\n",
    "    X_train, X_test, y_train, y_test = pp.split_data(dataset_t,r_state=r_state) # split dataset\n",
    "    \n",
    "    print('\\nCalculating Logistic Regression ...\\n')\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import binarize\n",
    "    \n",
    "    LR = LogisticRegression(random_state=r_state).fit(X_train, y_train) \n",
    "    \n",
    "    cutoff = 0.1840\n",
    "    y_pred = LR.predict_proba(X_test)\n",
    "    y_pred = binarize(y_pred,cutoff)[:,1]\n",
    "    \n",
    "    if v == 1:\n",
    "        print('Running Logistic Regression ...')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('\\nAccuracy is:', accuracy)\n",
    "        classification = classification_report(y_test, y_pred)\n",
    "        print(classification)\n",
    "        \n",
    "        print('Calculating metrics for ranging cutoff...')\n",
    "        results_LR = []\n",
    "        range_x = np.arange(0.05, 1, 0.02)\n",
    "        for cutoff in range_x:\n",
    "            results_LR.append(performance(LR, X_test, y_test, cutoff=cutoff, output='All'))\n",
    "        results_LR = pd.DataFrame(results_LR,columns=['accuracy','precision','recall','f1'])\n",
    "        \n",
    "\n",
    "    #probability score for ROC curve:    \n",
    "    y_pred_prob_yes=LR.predict_proba(X_test)\n",
    "    \n",
    "    #confusion matrix:\n",
    "    cm=confusion_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "    return results_LR, y_pred_prob_yes, y_test, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run all alogrithms on dataset \n",
    "resultsLR, ROC_LR, y_test_LR, cm_LR = LR_opt(dataset, v=1)\n",
    "resultsKN, ROC_KN, y_test_KN, cm_KN = k_neighbors_opt(dataset, v=1)\n",
    "resultsSVM, ROC_SVM, y_test_SVM, cm_SVM = SVM_opt(dataset, v=1)\n",
    "resultsMLP, ROC_MLP, y_test_MLP, cm_MLP = multilayer_opt(dataset, v=1)\n",
    "resultsRF, ROC_RF, y_test_RF, cm_RF = RF_opt(dataset, v=1)\n",
    "resultsDT, ROC_DT, y_test_DT, cm_DT = DT_opt(dataset, v=1)\n",
    "\n",
    "\n",
    "\n",
    "results_fram = fram_analysis(df_fram)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons: accuracy, precision, recall, f1_score, roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PLOTS \n",
    "plt.figure(figsize=(20, 10))\n",
    "range_x = np.arange(0.05, 1, 0.02)\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(range_x, resultsMLP['accuracy'], label='MLP')\n",
    "plt.plot(range_x, resultsSVM['accuracy'], label='SVM')\n",
    "plt.plot(range_x, resultsKN['accuracy'], label ='KN')\n",
    "plt.plot(range_x, resultsRF['accuracy'], label ='RF')\n",
    "plt.plot(range_x, resultsDT['accuracy'], label ='DT')\n",
    "plt.plot(range_x, resultsLR['accuracy'], label ='LR')\n",
    "plt.plot(range_x, results_fram['accuracy'], label ='fram' )\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Changing cutoff to see effect on accuracy')\n",
    "plt.xlabel('Cutoff')\n",
    "plt.ylabel('Accuracy Score')\n",
    "\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "#Precision Graph \n",
    "plt.plot(range_x, resultsMLP['precision'], label='MLP')\n",
    "plt.plot(range_x, resultsSVM['precision'], label='SVM')\n",
    "plt.plot(range_x, resultsKN['precision'], label ='KN') \n",
    "plt.plot(range_x, resultsRF['precision'], label ='RF')\n",
    "plt.plot(range_x, resultsDT['precision'], label ='DT')\n",
    "plt.plot(range_x, resultsLR['precision'], label ='LR')\n",
    "plt.plot(range_x, results_fram['precision'], label ='fram' )\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Changing cutoff to see effect on Precision')\n",
    "plt.xlabel('Cutoff')\n",
    "plt.ylabel('Precision Score')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "\n",
    "#Recall Graph \n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(range_x, resultsMLP['recall'], label='MLP')\n",
    "plt.plot(range_x, resultsSVM['recall'], label='SVM')\n",
    "plt.plot(range_x, resultsKN['recall'], label ='KN')\n",
    "plt.plot(range_x, resultsRF['recall'], label ='RF')\n",
    "plt.plot(range_x, resultsDT['recall'], label ='DT')\n",
    "plt.plot(range_x, resultsLR['recall'], label ='LR')\n",
    "plt.plot(range_x, results_fram['recall'], label ='fram' )\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Changing cutoff to see effect on Recall')\n",
    "plt.xlabel('Cutoff')\n",
    "plt.ylabel('Recall Score')\n",
    "\n",
    "#F1_score Graph \n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(range_x, resultsMLP['f1'], label='MLP')\n",
    "plt.plot(range_x, resultsSVM['f1'], label='SVM')\n",
    "plt.plot(range_x, resultsKN['f1'], label ='KN')\n",
    "plt.plot(range_x, resultsRF['f1'], label ='RF')\n",
    "plt.plot(range_x, resultsDT['f1'], label ='DT')\n",
    "plt.plot(range_x, resultsLR['f1'], label ='LR')\n",
    "plt.plot(range_x, results_fram['f1'], label ='fram' )\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Changing cutoff to see effect on F1_score')\n",
    "plt.xlabel('Cutoff')\n",
    "plt.ylabel('F1 Score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC CURVE\n",
    "plt.figure(figsize = (8,5))\n",
    "\n",
    "\n",
    "#K-Neighbors\n",
    "fpr, tpr, thresholds = roc_curve(y_test_KN, ROC_KN[:,1])\n",
    "plt.plot(fpr,tpr,label ='KN')\n",
    "print('roc auc for KN:',roc_auc_score(y_test_KN,ROC_KN[:,1]))\n",
    "\n",
    "#SVM\n",
    "fpr, tpr, thresholds = roc_curve(y_test_SVM, ROC_SVM[:,1])\n",
    "plt.plot(fpr,tpr,label ='SVM')\n",
    "print('roc auc for SVM:',roc_auc_score(y_test_SVM,ROC_SVM[:,1]))\n",
    "\n",
    "\n",
    "#MLP\n",
    "fpr, tpr, thresholds = roc_curve(y_test_MLP, ROC_MLP[:,1])\n",
    "plt.plot(fpr,tpr,label ='MLP')\n",
    "print('roc auc for MLP:',roc_auc_score(y_test_MLP,ROC_MLP[:,1]))\n",
    "\n",
    "#RF\n",
    "fpr, tpr, thresholds = roc_curve(y_test_RF, ROC_RF[:,1])\n",
    "plt.plot(fpr,tpr,label ='RF')\n",
    "print('roc auc for RF:',roc_auc_score(y_test_RF,ROC_RF[:,1]))\n",
    "\n",
    "#DT\n",
    "fpr, tpr, thresholds = roc_curve(y_test_DT, ROC_DT[:,1])\n",
    "plt.plot(fpr,tpr,label ='DT')\n",
    "print('roc auc for DT:',roc_auc_score(y_test_DT,ROC_DT[:,1]))\n",
    "\n",
    "#DT\n",
    "fpr, tpr, thresholds = roc_curve(y_test_LR, ROC_LR[:,1])\n",
    "plt.plot(fpr,tpr,label ='LR')\n",
    "print('roc auc for LR:',roc_auc_score(y_test_LR,ROC_LR[:,1]))\n",
    "\n",
    "#framingham dataset \n",
    "fpr, tpr, thresholds = roc_curve(df_fram.TenYearCHD, df_fram.fram_percent)\n",
    "plt.plot(fpr,tpr,label ='fram')\n",
    "print('roc auc for framingham:',roc_auc_score(df_fram.TenYearCHD, df_fram.fram_percent))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('ROC curves for all optimised algorithms')\n",
    "plt.xlabel('False positive rate (1-Specificity)')\n",
    "plt.ylabel('True positive rate (Sensitivity)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix for all alogrithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_KN  = pd.DataFrame(data=cm_KN,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "cm_SVM = pd.DataFrame(data=cm_SVM,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "cm_MLP = pd.DataFrame(data=cm_MLP,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "cm_RF = pd.DataFrame(data=cm_RF,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "cm_LR = pd.DataFrame(data=cm_LR,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(3,2,1)\n",
    "sn.heatmap(cm_KN, annot=True, fmt='d', cmap=\"YlGnBu\")\n",
    "plt.title('KN ')\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "sn.heatmap(cm_SVM, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "plt.title('SVM ')\n",
    "\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "sn.heatmap(cm_MLP, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "plt.title('MLP ')\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "sn.heatmap(cm_RF, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "plt.title('RF ')\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "sn.heatmap(cm_LR, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "plt.title('LR ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fram = fram_analysis(df_fram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
